{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\GradziPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GradziPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pymongo\n",
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(string):\n",
    "    #lower case\n",
    "    string =string.lower()\n",
    "    \n",
    "    #remove non letters\n",
    "    regex = re.compile('[^a-z ]')\n",
    "    string = regex.sub('', string)\n",
    "    \n",
    "    #break sentences into words and lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words_list = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else lemmatizer.lemmatize(i) for i,j in pos_tag(word_tokenize(string))]\n",
    "    \n",
    "    #remove english stopwords\n",
    "    stopword_list = stopwords.words('english')\n",
    "    stopword_list.append('rt')\n",
    "    stopword_list = set(stopword_list)\n",
    "    words_list = [w for w in words_list if not w in stopword_list]\n",
    "    #add rt to \n",
    "    \n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['Assignment3']\n",
    "collection = db['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GradziPC\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "tweets = collection.find(projection={'text'})\n",
    "tweets_list = []\n",
    "for i in range(tweets.count()):\n",
    "    t = tweets.next()['text']\n",
    "    t = preprocess(t)\n",
    "    tweets_list.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calclate document frequency per word\n",
    "def calc_token_frequencies(doc_list):\n",
    "    frequencies = defaultdict(int)# Each dict item will start off as int(0)\n",
    "    \n",
    "    for token_set in doc_list:\n",
    "        frequencies_tw = {}\n",
    "        for token in token_set:\n",
    "            frequencies_tw[token] = 1\n",
    "        for tok in frequencies_tw:\n",
    "            frequencies[tok] += 1\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove words that occur in less than 10 documents, and words that occur in more than 90% of the documents\n",
    "words_count = calc_token_frequencies(tweets_list)\n",
    "filted_dict = {k:v for k,v in words_count.items() if v>9 and v<len(tweets_list)*0.9}\n",
    "\n",
    "#Transform each document to a vectorized form by computing the frequency of each word. (50000x5104 vector)\n",
    "tweets_vect = np.zeros((len(filted_dict),len(tweets_list)))\n",
    "for i in range(len(tweets_list)):\n",
    "    tw_dict = dict.fromkeys(filted_dict,0)\n",
    "    for word in tweets_list[i]:\n",
    "        if word in tw_dict:\n",
    "            tw_dict[word] +=1\n",
    "    tweets_vect[:,i] = [int(a) for a in list(tw_dict.values())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 50000\n",
      "INFO:lda:vocab_size: 4735\n",
      "INFO:lda:n_words: 440360\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 1500\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -5226611\n",
      "INFO:lda:<10> log likelihood: -3220307\n",
      "INFO:lda:<20> log likelihood: -3052857\n",
      "INFO:lda:<30> log likelihood: -3023662\n",
      "INFO:lda:<40> log likelihood: -3011148\n",
      "INFO:lda:<50> log likelihood: -3003903\n",
      "INFO:lda:<60> log likelihood: -2999472\n",
      "INFO:lda:<70> log likelihood: -2994881\n",
      "INFO:lda:<80> log likelihood: -2992297\n",
      "INFO:lda:<90> log likelihood: -2990867\n",
      "INFO:lda:<100> log likelihood: -2987660\n",
      "INFO:lda:<110> log likelihood: -2986675\n",
      "INFO:lda:<120> log likelihood: -2983914\n",
      "INFO:lda:<130> log likelihood: -2982660\n",
      "INFO:lda:<140> log likelihood: -2979963\n",
      "INFO:lda:<150> log likelihood: -2978323\n",
      "INFO:lda:<160> log likelihood: -2977262\n",
      "INFO:lda:<170> log likelihood: -2977210\n",
      "INFO:lda:<180> log likelihood: -2977489\n",
      "INFO:lda:<190> log likelihood: -2975944\n",
      "INFO:lda:<200> log likelihood: -2974675\n",
      "INFO:lda:<210> log likelihood: -2974509\n",
      "INFO:lda:<220> log likelihood: -2974832\n",
      "INFO:lda:<230> log likelihood: -2973934\n",
      "INFO:lda:<240> log likelihood: -2974118\n",
      "INFO:lda:<250> log likelihood: -2972941\n",
      "INFO:lda:<260> log likelihood: -2973093\n",
      "INFO:lda:<270> log likelihood: -2972714\n",
      "INFO:lda:<280> log likelihood: -2972706\n",
      "INFO:lda:<290> log likelihood: -2971830\n",
      "INFO:lda:<300> log likelihood: -2972454\n",
      "INFO:lda:<310> log likelihood: -2971026\n",
      "INFO:lda:<320> log likelihood: -2971063\n",
      "INFO:lda:<330> log likelihood: -2969676\n",
      "INFO:lda:<340> log likelihood: -2970532\n",
      "INFO:lda:<350> log likelihood: -2970372\n",
      "INFO:lda:<360> log likelihood: -2970410\n",
      "INFO:lda:<370> log likelihood: -2968307\n",
      "INFO:lda:<380> log likelihood: -2969788\n",
      "INFO:lda:<390> log likelihood: -2969036\n",
      "INFO:lda:<400> log likelihood: -2969324\n",
      "INFO:lda:<410> log likelihood: -2969558\n",
      "INFO:lda:<420> log likelihood: -2968693\n",
      "INFO:lda:<430> log likelihood: -2968439\n",
      "INFO:lda:<440> log likelihood: -2968280\n",
      "INFO:lda:<450> log likelihood: -2968624\n",
      "INFO:lda:<460> log likelihood: -2967843\n",
      "INFO:lda:<470> log likelihood: -2967903\n",
      "INFO:lda:<480> log likelihood: -2968260\n",
      "INFO:lda:<490> log likelihood: -2967170\n",
      "INFO:lda:<500> log likelihood: -2967369\n",
      "INFO:lda:<510> log likelihood: -2967752\n",
      "INFO:lda:<520> log likelihood: -2967192\n",
      "INFO:lda:<530> log likelihood: -2967473\n",
      "INFO:lda:<540> log likelihood: -2966898\n",
      "INFO:lda:<550> log likelihood: -2967160\n",
      "INFO:lda:<560> log likelihood: -2966807\n",
      "INFO:lda:<570> log likelihood: -2966826\n",
      "INFO:lda:<580> log likelihood: -2966930\n",
      "INFO:lda:<590> log likelihood: -2966853\n",
      "INFO:lda:<600> log likelihood: -2966132\n",
      "INFO:lda:<610> log likelihood: -2966394\n",
      "INFO:lda:<620> log likelihood: -2966511\n",
      "INFO:lda:<630> log likelihood: -2966247\n",
      "INFO:lda:<640> log likelihood: -2965940\n",
      "INFO:lda:<650> log likelihood: -2965160\n",
      "INFO:lda:<660> log likelihood: -2965850\n",
      "INFO:lda:<670> log likelihood: -2965318\n",
      "INFO:lda:<680> log likelihood: -2964664\n",
      "INFO:lda:<690> log likelihood: -2965362\n",
      "INFO:lda:<700> log likelihood: -2965422\n",
      "INFO:lda:<710> log likelihood: -2964627\n",
      "INFO:lda:<720> log likelihood: -2964835\n",
      "INFO:lda:<730> log likelihood: -2964936\n",
      "INFO:lda:<740> log likelihood: -2965019\n",
      "INFO:lda:<750> log likelihood: -2965165\n",
      "INFO:lda:<760> log likelihood: -2965055\n",
      "INFO:lda:<770> log likelihood: -2965290\n",
      "INFO:lda:<780> log likelihood: -2965517\n",
      "INFO:lda:<790> log likelihood: -2964493\n",
      "INFO:lda:<800> log likelihood: -2965362\n",
      "INFO:lda:<810> log likelihood: -2964607\n",
      "INFO:lda:<820> log likelihood: -2964615\n",
      "INFO:lda:<830> log likelihood: -2963560\n",
      "INFO:lda:<840> log likelihood: -2963372\n",
      "INFO:lda:<850> log likelihood: -2964192\n",
      "INFO:lda:<860> log likelihood: -2963435\n",
      "INFO:lda:<870> log likelihood: -2964498\n",
      "INFO:lda:<880> log likelihood: -2963109\n",
      "INFO:lda:<890> log likelihood: -2964330\n",
      "INFO:lda:<900> log likelihood: -2963433\n",
      "INFO:lda:<910> log likelihood: -2963463\n",
      "INFO:lda:<920> log likelihood: -2964058\n",
      "INFO:lda:<930> log likelihood: -2964060\n",
      "INFO:lda:<940> log likelihood: -2964203\n",
      "INFO:lda:<950> log likelihood: -2963459\n",
      "INFO:lda:<960> log likelihood: -2962733\n",
      "INFO:lda:<970> log likelihood: -2962259\n",
      "INFO:lda:<980> log likelihood: -2963545\n",
      "INFO:lda:<990> log likelihood: -2963124\n",
      "INFO:lda:<1000> log likelihood: -2964360\n",
      "INFO:lda:<1010> log likelihood: -2963936\n",
      "INFO:lda:<1020> log likelihood: -2963488\n",
      "INFO:lda:<1030> log likelihood: -2963035\n",
      "INFO:lda:<1040> log likelihood: -2962842\n",
      "INFO:lda:<1050> log likelihood: -2963662\n",
      "INFO:lda:<1060> log likelihood: -2963698\n",
      "INFO:lda:<1070> log likelihood: -2962495\n",
      "INFO:lda:<1080> log likelihood: -2962817\n",
      "INFO:lda:<1090> log likelihood: -2963580\n",
      "INFO:lda:<1100> log likelihood: -2962949\n",
      "INFO:lda:<1110> log likelihood: -2963796\n",
      "INFO:lda:<1120> log likelihood: -2963582\n",
      "INFO:lda:<1130> log likelihood: -2963157\n",
      "INFO:lda:<1140> log likelihood: -2963698\n",
      "INFO:lda:<1150> log likelihood: -2964224\n",
      "INFO:lda:<1160> log likelihood: -2963477\n",
      "INFO:lda:<1170> log likelihood: -2963119\n",
      "INFO:lda:<1180> log likelihood: -2963860\n",
      "INFO:lda:<1190> log likelihood: -2962661\n",
      "INFO:lda:<1200> log likelihood: -2962598\n",
      "INFO:lda:<1210> log likelihood: -2963316\n",
      "INFO:lda:<1220> log likelihood: -2962488\n",
      "INFO:lda:<1230> log likelihood: -2963191\n",
      "INFO:lda:<1240> log likelihood: -2962971\n",
      "INFO:lda:<1250> log likelihood: -2963355\n",
      "INFO:lda:<1260> log likelihood: -2962262\n",
      "INFO:lda:<1270> log likelihood: -2962915\n",
      "INFO:lda:<1280> log likelihood: -2962961\n",
      "INFO:lda:<1290> log likelihood: -2961192\n",
      "INFO:lda:<1300> log likelihood: -2962448\n",
      "INFO:lda:<1310> log likelihood: -2962488\n",
      "INFO:lda:<1320> log likelihood: -2962431\n",
      "INFO:lda:<1330> log likelihood: -2961826\n",
      "INFO:lda:<1340> log likelihood: -2961483\n",
      "INFO:lda:<1350> log likelihood: -2963175\n",
      "INFO:lda:<1360> log likelihood: -2962617\n",
      "INFO:lda:<1370> log likelihood: -2961574\n",
      "INFO:lda:<1380> log likelihood: -2962795\n",
      "INFO:lda:<1390> log likelihood: -2962294\n",
      "INFO:lda:<1400> log likelihood: -2962660\n",
      "INFO:lda:<1410> log likelihood: -2961620\n",
      "INFO:lda:<1420> log likelihood: -2962364\n",
      "INFO:lda:<1430> log likelihood: -2963243\n",
      "INFO:lda:<1440> log likelihood: -2962662\n",
      "INFO:lda:<1450> log likelihood: -2961851\n",
      "INFO:lda:<1460> log likelihood: -2960948\n",
      "INFO:lda:<1470> log likelihood: -2962432\n",
      "INFO:lda:<1480> log likelihood: -2961303\n",
      "INFO:lda:<1490> log likelihood: -2961237\n",
      "INFO:lda:<1499> log likelihood: -2961551\n"
     ]
    }
   ],
   "source": [
    "#Apply the LDA model.\n",
    "#Examine the topics, with your own proposal for the “name” of each topic that is meaningful to you. Display your results in the notebook.\n",
    "import lda\n",
    "\n",
    "X= tweets_vect.astype(int)\n",
    "X = X.transpose()\n",
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
    "model.fit(X)  # model.fit_transform(X) is also available\n",
    "topic_word = model.topic_word_  # model.components_ also works\n",
    "n_top_words = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: covid get home spread go\n",
      "Topic 1: amp use pandemic take update\n",
      "Topic 2: uk death break piersmorgan spike\n",
      "Topic 3: watch covid pandemic brief point\n",
      "Topic 4: china covid day wuhan say\n",
      "Topic 5: help mask fight combat amp\n",
      "Topic 6: death u rate die south\n",
      "Topic 7: death report may break covid\n",
      "Topic 8: covid home stay know life\n",
      "Topic 9: people die help lockdown worker\n",
      "Topic 10: covid time week http life\n",
      "Topic 11: covid fight help quicktake pandemic\n",
      "Topic 12: pandemic covid impact support crisis\n",
      "Topic 13: case new covid death total\n",
      "Topic 14: latest daily thanks day american\n",
      "Topic 15: test state china warn could\n",
      "Topic 16: get tomfitton public realdonaldtrump shutdown\n",
      "Topic 17: care boris johnson minister intensive\n",
      "Topic 18: crisis people continue current accelerate\n",
      "Topic 19: trump medical president staff nh\n"
     ]
    }
   ],
   "source": [
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(tuple(filted_dict))[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results with title\n",
    "\n",
    "#### Topic Go home\" :              covid get home spread go\n",
    "\n",
    "Topic **Update**:               amp use pandemic take update\n",
    "\n",
    "Topic **UK spike**:             uk death break piersmorgan spike\n",
    "   \n",
    "Topic **Quick stats**:          watch covid pandemic brief point\n",
    "\n",
    "Topic **Wuhan discharged**:     china covid day wuhan say\n",
    "\n",
    "Topic **Mask**:                 help mask fight combat amp\n",
    "\n",
    "Topic **Death Rate**:           death u rate die south\n",
    "\n",
    "Topic **May report**:           death report may break covid\n",
    "\n",
    "Topic **Stay@home**:            covid home stay know life\n",
    "\n",
    "Topic **Lockdown&workers**:     people die help lockdown worker\n",
    "\n",
    "Topic **Times**:                covid time week http life\n",
    "\n",
    "Topic **Fight covid**:          covid fight help quicktake pandemic\n",
    "\n",
    "Topic **Covid impact**:         pandemic covid impact support crisis\n",
    "\n",
    "Topic **Covid stats**:          case new covid death total\n",
    "\n",
    "Topic **Thanks day**:           latest daily thanks day american\n",
    "\n",
    "Topic **China's late warning**: test state china warn could\n",
    "\n",
    "Topic **Tim Fitton**:           get tomfitton public realdonaldtrump shutdown\n",
    "\n",
    "Topic **Boris johnson**:        care boris johnson minister intensive\n",
    "\n",
    "Topic **Crisis**:         people continue current accelerate\n",
    "\n",
    "Topic **Trump & medical staff**:      trump medical president staff nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
